{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(file_data,all_words):\n",
    "    for word in file_data.split():\n",
    "            if(len(word)>2):\n",
    "                all_words[word]=all_words.get(word,0)+1\n",
    "    return all_words\n",
    "\n",
    "def process_file_data(file_data):\n",
    "        file_data=file_data.replace(\"[\",\" \")\n",
    "        file_data=file_data.replace(\"]\",\" \")\n",
    "        file_data=file_data.replace(\"\\n\",\" \")\n",
    "        file_data=file_data.replace(\"-\",\" \")\n",
    "        file_data=file_data.replace(\"'\",\" \")\n",
    "        file_data=re.sub('[:<>}/\\\\''^&;\"\",)*_#=(?.!|]', ' ', file_data)\n",
    "        file_data=file_data.lower() #to lower case\n",
    "        \n",
    "        file_data=' '.join(file_data.split())#remove extras spaces in betwen\n",
    "        #print(file_data)\n",
    "        return file_data\n",
    "\n",
    "def remove_stopwords(all_words):\n",
    "    stopwords_1=stopwords.words('english')\n",
    "    stopwords_2=['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves']\n",
    "    stopwords_3=[\"edu\",\"cmu\",\"apr\",\"date\",\"newsgroups\",\"organization\",\"writes\",\"about\",\n",
    "                    \"article\",\"misc\",\"than\",\"com\",\"srv\",\"cantaloupe\",\"das\",]\n",
    "    stopwords_list=[stopwords_1,stopwords_2,stopwords_3]\n",
    "    for curr_list in stopwords_list:\n",
    "        for curr_word in curr_list:\n",
    "            if curr_word in all_words.keys():\n",
    "                del all_words[curr_word]\n",
    "  \n",
    "    all_words=sorted(all_words.items(), key=lambda x: x[1],reverse=True)\n",
    "    return all_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words={}\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "rootdir='C:\\\\Users\\\\Dhivya S\\\\Videos\\\\Captures\\\\TextClassification\\\\20_newsgroups'\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    category=subdir.replace(rootdir+\"\\\\\",\"\") # y value for corresponsing files\n",
    "    for file in files:\n",
    "        file_location=os.path.join(subdir, file)\n",
    "        file_obj=open(file_location,'r')\n",
    "        file_data=file_obj.read()\n",
    "        file_data=process_file_data(file_data)\n",
    "        all_words=make_dict(file_data,all_words) \n",
    "useful_words=remove_stopwords(all_words)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29e999e82c8>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vocabulary=[]\n",
    "pts=[]\n",
    "for i in range(3500): #choosing top 3500 words as vocabulary\n",
    "    vocabulary.append(useful_words[i][0])\n",
    "    pts.append(useful_words[i][1])\n",
    "plt.plot(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a base of first numpy array for data points and output\n",
    "first_file_loc='C:\\\\Users\\\\Dhivya S\\\\Videos\\\\Captures\\\\TextClassification\\\\20_newsgroups\\\\alt.atheism\\\\49960'\n",
    "file_obj=open(first_file_loc,'r')\n",
    "file_data=file_obj.read()\n",
    "file_data=process_file_data(file_data)\n",
    "x=[]\n",
    "for vocab in vocabulary:\n",
    "    count=file_data.count(' '+vocab+' ') #find the frequency of the word in the file and append the count\n",
    "    x.append(count)             \n",
    "y_list=['alt.atheism'] #base for output data numpy array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rootdir='C:\\\\Users\\\\Dhivya S\\\\Videos\\\\Captures\\\\TextClassification\\\\20_newsgroups'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    category=subdir.replace(rootdir+\"\\\\\",\"\") # y value for corresponsing files\n",
    "    for file in files:\n",
    "        y_list.append(category)\n",
    "        file_location=os.path.join(subdir, file)\n",
    "        file_obj=open(file_location,'r')\n",
    "        file_data=file_obj.read()\n",
    "        file_data=process_file_data(file_data)\n",
    "        x1=[]\n",
    "        for vocab in vocabulary:\n",
    "            count=file_data.count(' '+vocab+' ')\n",
    "            x1.append(count) \n",
    "        x=np.vstack([x,x1])\n",
    "y=np.array(y_list)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19998, 3500), 19998)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,len(y) #shape of data numpy arrays and output numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 19, 19, 19], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "#list(le.classes_)  \n",
    "y=le.transform(y)  #to encode the class names (strings) to integers for feeding into the model\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(x,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTING MULTINOMIAL NAIVE BAYES FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train,y_train): \n",
    "    result={}\n",
    "    class_values=set(y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"]=len(y_train)\n",
    "        current_class_rows=(y_train==current_class)\n",
    "        x_train_current=x_train[current_class_rows]\n",
    "        y_train_current=y_train[current_class_rows]\n",
    "        num_features = x_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(y_train_current)  #total rows present for given class\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] =x_train_current[:,j-1].sum()    #sum of each feature of the given class =(sj)      \n",
    "    return result  \n",
    "\n",
    "def probability(dictionary, x, current_class):\n",
    "    log_prob = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys())-2 \n",
    "    for j in range(1, num_features):\n",
    "        word_count_for_feature=dictionary[current_class][j]+1 #retrieving sum of ocuurances of the particular vocabulary/feature\n",
    "        total_rows_in_class=dictionary[current_class][\"total_count\"]+num_features\n",
    "        current_feature_probablity = np.log(word_count_for_feature) - np.log(total_rows_in_class)\n",
    "        log_prob=log_prob + current_feature_probablity*x[j - 1]       \n",
    "    return log_prob\n",
    "\n",
    "def predictSingleFile(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run) or (p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class\n",
    "\n",
    "def predict(dictionary, x_test):\n",
    "    y_pred = []\n",
    "    for x in x_test:\n",
    "        x_class = predictSingleFile(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted\n",
      "Predicted\n"
     ]
    }
   ],
   "source": [
    "dictionary = fit(X_train,Y_train)\n",
    "print(\"Fitted\")\n",
    "Y_pred = predict(dictionary,X_test)\n",
    "print(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81       233\n",
      "           1       0.70      0.82      0.76       261\n",
      "           2       0.94      0.53      0.68       257\n",
      "           3       0.78      0.81      0.79       242\n",
      "           4       0.91      0.82      0.86       268\n",
      "           5       0.62      0.85      0.72       232\n",
      "           6       0.94      0.71      0.81       277\n",
      "           7       0.93      0.84      0.88       266\n",
      "           8       0.97      0.85      0.91       253\n",
      "           9       1.00      0.85      0.92       250\n",
      "          10       0.91      0.98      0.94       257\n",
      "          11       0.78      1.00      0.87       240\n",
      "          12       0.94      0.63      0.75       256\n",
      "          13       0.96      0.89      0.92       219\n",
      "          14       0.80      0.92      0.86       234\n",
      "          15       0.97      1.00      0.99       261\n",
      "          16       0.82      0.84      0.83       261\n",
      "          17       0.68      0.97      0.80       228\n",
      "          18       0.64      0.84      0.72       259\n",
      "          19       0.71      0.55      0.62       246\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.84      0.83      0.82      5000\n",
      "weighted avg       0.84      0.82      0.82      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTINOMIAL BAYES USING SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       233\n",
      "           1       0.76      0.77      0.77       261\n",
      "           2       0.90      0.71      0.79       257\n",
      "           3       0.72      0.83      0.77       242\n",
      "           4       0.79      0.89      0.83       268\n",
      "           5       0.81      0.74      0.77       232\n",
      "           6       0.88      0.91      0.89       277\n",
      "           7       0.90      0.94      0.92       266\n",
      "           8       0.92      0.97      0.95       253\n",
      "           9       0.96      0.95      0.96       250\n",
      "          10       0.97      0.95      0.96       257\n",
      "          11       0.99      0.95      0.97       240\n",
      "          12       0.91      0.91      0.91       256\n",
      "          13       0.98      0.90      0.94       219\n",
      "          14       0.94      0.96      0.95       234\n",
      "          15       0.97      1.00      0.99       261\n",
      "          16       0.80      0.94      0.86       261\n",
      "          17       0.92      0.88      0.90       228\n",
      "          18       0.79      0.73      0.76       259\n",
      "          19       0.74      0.58      0.65       246\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy of inbuilt sklearn's MultinomialNB on the same data is 0.87\n",
      "Our accuarcy is 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuaracy of inbuilt sklearn's MultinomialNB on the same data is 0.87\")\n",
    "print(\"Our accuarcy is 0.82\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
